{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9388e89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../stylegan3')\n",
    "\n",
    "import torch\n",
    "from latents_dataset import LatentsDataset\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from models.LatentsAutoencoder import LatentsAutoencoder\n",
    "from pytorch_nn import NNUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e39d0b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/robert/data/diploma-thesis/datasets/stylegan3/tpsi_1/latents/sample_z.h5'\n",
    "targets_path = '/home/robert/data/diploma-thesis/predictions/stylegan3/tpsi_1/resnet34_eyeglasses.pkl'\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6e0b10f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'latents_dataset' has no attribute 'load_autoencoder_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([])\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mlatents_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_autoencoder_dataset\u001b[49m(data_path,transform\u001b[38;5;241m=\u001b[39mtransform,minmax_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m train_data, valid_data, test_data \u001b[38;5;241m=\u001b[39m random_split(dataset, [\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.1\u001b[39m], generator\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mGenerator()\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m42\u001b[39m))\n\u001b[1;32m      5\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_data, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'latents_dataset' has no attribute 'load_autoencoder_dataset'"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([])\n",
    "ld = LatentsDataset(data_path, targets_path)\n",
    "dataset = latents_dataset.load_autoencoder_dataset(transform=transform, minmax_norm = True)\n",
    "train_data, valid_data, test_data = random_split(dataset, [0.8, 0.1, 0.1], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f'Shape of X [N, C, H, W]: {X.shape}', X)\n",
    "    print(f'Shape of y: {y.shape} {y.dtype}', y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc840ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (batch_size, len(dataset[0][0]))\n",
    "model = LatentsAutoencoder(input_shape=input_shape)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-1, weight_decay=1e-8)\n",
    "dataloaders={'train': train_dataloader, 'valid': valid_dataloader, 'test': test_dataloader}\n",
    "save_filename = 'latent_encoder_ver1.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b91a69ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = NNUtil(model=model, dataloaders=dataloaders, loss_fn=loss_fn, optimizer=optimizer, save_filename=save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6efa2f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.225643  [    0/204800] time: 0.8601269721984863\n",
      "loss: 0.015259  [ 6400/204800] time: 0.7688915729522705\n",
      "loss: 0.011921  [12800/204800] time: 0.8236956596374512\n",
      "loss: 0.011966  [19200/204800] time: 0.7864189147949219\n",
      "loss: 0.012008  [25600/204800] time: 0.730626106262207\n",
      "loss: 0.012030  [32000/204800] time: 0.7856941223144531\n",
      "loss: 0.012011  [38400/204800] time: 0.7081389427185059\n",
      "loss: 0.012045  [44800/204800] time: 0.702704906463623\n",
      "loss: 0.012194  [51200/204800] time: 0.7324161529541016\n",
      "loss: 0.012137  [57600/204800] time: 0.7266449928283691\n",
      "loss: 0.012008  [64000/204800] time: 0.7133946418762207\n",
      "loss: 0.012098  [70400/204800] time: 0.7718245983123779\n",
      "loss: 0.012063  [76800/204800] time: 0.6066725254058838\n",
      "loss: 0.012145  [83200/204800] time: 0.45127177238464355\n",
      "loss: 0.012083  [89600/204800] time: 0.4631516933441162\n",
      "loss: 0.012214  [96000/204800] time: 0.4544825553894043\n",
      "loss: 0.012094  [102400/204800] time: 0.4581911563873291\n",
      "loss: 0.012099  [108800/204800] time: 0.4698913097381592\n",
      "loss: 0.012264  [115200/204800] time: 0.6488091945648193\n",
      "loss: 0.012154  [121600/204800] time: 0.74698805809021\n",
      "loss: 0.012166  [128000/204800] time: 0.8437380790710449\n",
      "loss: 0.012055  [134400/204800] time: 0.7376420497894287\n",
      "loss: 0.012199  [140800/204800] time: 0.7801682949066162\n",
      "loss: 0.012323  [147200/204800] time: 0.7328295707702637\n",
      "loss: 0.012339  [153600/204800] time: 0.7015290260314941\n",
      "loss: 0.012423  [160000/204800] time: 0.7672009468078613\n",
      "loss: 0.012264  [166400/204800] time: 0.7486157417297363\n",
      "loss: 0.012444  [172800/204800] time: 0.7393088340759277\n",
      "loss: 0.012419  [179200/204800] time: 0.7419233322143555\n",
      "loss: 0.012510  [185600/204800] time: 0.7695596218109131\n",
      "loss: 0.012644  [192000/204800] time: 0.7918720245361328\n",
      "loss: 0.012529  [198400/204800] time: 0.7659854888916016\n",
      "Valid | Error: \n",
      " Avg loss: 0.012678 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.012664  [    0/204800] time: 0.0142669677734375\n",
      "loss: 0.012733  [ 6400/204800] time: 0.7550234794616699\n",
      "loss: 0.014038  [12800/204800] time: 0.7948005199432373\n",
      "loss: 0.015656  [19200/204800] time: 0.7743706703186035\n",
      "loss: 0.014490  [25600/204800] time: 0.7580296993255615\n",
      "loss: 0.012784  [32000/204800] time: 0.6528294086456299\n",
      "loss: 0.012585  [38400/204800] time: 0.7510552406311035\n",
      "loss: 0.012555  [44800/204800] time: 0.7091872692108154\n",
      "loss: 0.912879  [51200/204800] time: 0.6978363990783691\n",
      "loss: 0.012007  [57600/204800] time: 0.7241666316986084\n",
      "loss: 0.012080  [64000/204800] time: 0.7820324897766113\n",
      "loss: 0.012121  [70400/204800] time: 0.7704339027404785\n",
      "loss: 0.012000  [76800/204800] time: 0.7307007312774658\n",
      "loss: 0.012113  [83200/204800] time: 0.7302083969116211\n",
      "loss: 0.011968  [89600/204800] time: 0.7960491180419922\n",
      "loss: 0.012026  [96000/204800] time: 0.7473862171173096\n",
      "loss: 0.011924  [102400/204800] time: 0.6750423908233643\n",
      "loss: 0.011921  [108800/204800] time: 0.7530426979064941\n",
      "loss: 0.012167  [115200/204800] time: 0.7501921653747559\n",
      "loss: 0.011994  [121600/204800] time: 0.7683541774749756\n",
      "loss: 0.011992  [128000/204800] time: 0.7393069267272949\n",
      "loss: 0.142907  [134400/204800] time: 0.7443804740905762\n",
      "loss: 0.023210  [140800/204800] time: 0.7731060981750488\n",
      "loss: 0.012592  [147200/204800] time: 0.7405693531036377\n",
      "loss: 0.013189  [153600/204800] time: 0.7444255352020264\n",
      "loss: 0.074224  [160000/204800] time: 0.7399611473083496\n",
      "loss: 0.012036  [166400/204800] time: 0.7342782020568848\n",
      "loss: 0.384293  [172800/204800] time: 0.716275691986084\n",
      "loss: 0.013416  [179200/204800] time: 0.7175302505493164\n",
      "loss: 0.012050  [185600/204800] time: 0.7130122184753418\n",
      "loss: 0.013215  [192000/204800] time: 0.7341670989990234\n",
      "loss: 0.014739  [198400/204800] time: 0.7422947883605957\n",
      "Valid | Error: \n",
      " Avg loss: 0.012081 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.012071  [    0/204800] time: 0.014475345611572266\n",
      "loss: 0.017849  [ 6400/204800] time: 0.7294402122497559\n",
      "loss: 0.207030  [12800/204800] time: 0.7414228916168213\n",
      "loss: 0.012054  [19200/204800] time: 0.7307171821594238\n",
      "loss: 0.013358  [25600/204800] time: 0.7507915496826172\n",
      "loss: 0.012518  [32000/204800] time: 0.7044408321380615\n",
      "loss: 0.012144  [38400/204800] time: 0.711920976638794\n",
      "loss: 0.063841  [44800/204800] time: 0.7087142467498779\n",
      "loss: 0.011929  [51200/204800] time: 0.7138631343841553\n",
      "loss: 0.012273  [57600/204800] time: 0.71018385887146\n",
      "loss: 0.177679  [64000/204800] time: 0.7105755805969238\n",
      "loss: 0.012213  [70400/204800] time: 0.7104732990264893\n",
      "loss: 0.033080  [76800/204800] time: 0.809227705001831\n",
      "loss: 0.045165  [83200/204800] time: 0.6850864887237549\n",
      "loss: 0.012342  [89600/204800] time: 0.6353814601898193\n",
      "loss: 0.012225  [96000/204800] time: 0.560969352722168\n",
      "loss: 0.012668  [102400/204800] time: 0.550581693649292\n",
      "loss: 0.014880  [108800/204800] time: 0.4609527587890625\n",
      "loss: 0.011980  [115200/204800] time: 0.5639402866363525\n",
      "loss: 0.013720  [121600/204800] time: 0.6997976303100586\n",
      "loss: 0.067678  [128000/204800] time: 0.4926133155822754\n",
      "loss: 0.012435  [134400/204800] time: 0.4646127223968506\n",
      "loss: 0.012741  [140800/204800] time: 0.4544684886932373\n",
      "loss: 0.012350  [147200/204800] time: 0.45743322372436523\n",
      "loss: 0.025599  [153600/204800] time: 0.45984578132629395\n",
      "loss: 0.012216  [160000/204800] time: 0.46053409576416016\n",
      "loss: 0.012334  [166400/204800] time: 0.45880937576293945\n",
      "loss: 0.012216  [172800/204800] time: 0.4585709571838379\n",
      "loss: 0.016740  [179200/204800] time: 0.46060752868652344\n",
      "loss: 0.028185  [185600/204800] time: 0.4574706554412842\n",
      "loss: 0.018907  [192000/204800] time: 0.462371826171875\n",
      "loss: 0.288822  [198400/204800] time: 0.45703768730163574\n",
      "Valid | Error: \n",
      " Avg loss: 0.012282 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.012392  [    0/204800] time: 0.013653278350830078\n",
      "loss: 0.112657  [ 6400/204800] time: 0.7362174987792969\n",
      "loss: 0.012386  [12800/204800] time: 0.7648603916168213\n",
      "loss: 0.012298  [19200/204800] time: 0.7510263919830322\n",
      "loss: 0.012620  [25600/204800] time: 0.7061238288879395\n",
      "loss: 0.117352  [32000/204800] time: 0.7089314460754395\n",
      "loss: 0.012170  [38400/204800] time: 0.5226013660430908\n",
      "loss: 0.012155  [44800/204800] time: 0.459453821182251\n",
      "loss: 0.011898  [51200/204800] time: 0.4581716060638428\n",
      "loss: 0.012217  [57600/204800] time: 0.46478724479675293\n",
      "loss: 0.012026  [64000/204800] time: 0.5778746604919434\n",
      "loss: 0.012104  [70400/204800] time: 0.6953001022338867\n",
      "loss: 0.012193  [76800/204800] time: 0.5704996585845947\n",
      "loss: 0.012202  [83200/204800] time: 0.46039748191833496\n",
      "loss: 0.012365  [89600/204800] time: 0.4610624313354492\n",
      "loss: 0.012229  [96000/204800] time: 0.5246009826660156\n",
      "loss: 0.012448  [102400/204800] time: 0.6624529361724854\n",
      "loss: 0.012506  [108800/204800] time: 0.7424812316894531\n",
      "loss: 0.012588  [115200/204800] time: 0.7618353366851807\n",
      "loss: 0.012256  [121600/204800] time: 0.7313284873962402\n",
      "loss: 0.090570  [128000/204800] time: 0.7127776145935059\n",
      "loss: 0.012797  [134400/204800] time: 0.7516617774963379\n",
      "loss: 0.013421  [140800/204800] time: 0.7675762176513672\n",
      "loss: 0.012298  [147200/204800] time: 0.5467078685760498\n",
      "loss: 0.012379  [153600/204800] time: 0.45725131034851074\n",
      "loss: 0.043895  [160000/204800] time: 0.45829033851623535\n",
      "loss: 0.012424  [166400/204800] time: 0.45876407623291016\n",
      "loss: 0.012523  [172800/204800] time: 0.47190213203430176\n",
      "loss: 0.012373  [179200/204800] time: 0.4638092517852783\n",
      "loss: 0.013165  [185600/204800] time: 0.46836376190185547\n",
      "loss: 0.012489  [192000/204800] time: 0.46564388275146484\n",
      "loss: 0.012296  [198400/204800] time: 0.46178579330444336\n",
      "Valid | Error: \n",
      " Avg loss: 0.012396 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.012285  [    0/204800] time: 0.011336565017700195\n",
      "loss: 0.013213  [ 6400/204800] time: 0.4643828868865967\n",
      "loss: 0.057926  [12800/204800] time: 0.4620802402496338\n",
      "loss: 0.013715  [19200/204800] time: 0.4588758945465088\n",
      "loss: 0.012381  [25600/204800] time: 0.45949769020080566\n",
      "loss: 0.013144  [32000/204800] time: 0.4596383571624756\n",
      "loss: 0.019437  [38400/204800] time: 0.46059679985046387\n",
      "loss: 0.012284  [44800/204800] time: 0.5264937877655029\n",
      "loss: 0.012375  [51200/204800] time: 0.4598979949951172\n",
      "loss: 0.012511  [57600/204800] time: 0.6177108287811279\n",
      "loss: 0.012466  [64000/204800] time: 0.4888031482696533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.012253  [70400/204800] time: 0.46683716773986816\n",
      "loss: 0.012400  [76800/204800] time: 0.4589564800262451\n",
      "loss: 0.012544  [83200/204800] time: 0.4586215019226074\n",
      "loss: 0.012456  [89600/204800] time: 0.45899486541748047\n",
      "loss: 0.012718  [96000/204800] time: 0.5679900646209717\n",
      "loss: 0.013292  [102400/204800] time: 0.6954116821289062\n",
      "loss: 0.012547  [108800/204800] time: 0.7035994529724121\n",
      "loss: 0.012710  [115200/204800] time: 0.6982681751251221\n",
      "loss: 0.013029  [121600/204800] time: 0.6938214302062988\n",
      "loss: 0.017910  [128000/204800] time: 0.46205925941467285\n",
      "loss: 0.022601  [134400/204800] time: 0.5023605823516846\n",
      "loss: 0.016886  [140800/204800] time: 0.5524942874908447\n",
      "loss: 0.015685  [147200/204800] time: 0.45740365982055664\n",
      "loss: 0.013223  [153600/204800] time: 0.5668797492980957\n",
      "loss: 0.013553  [160000/204800] time: 0.6325058937072754\n",
      "loss: 0.038368  [166400/204800] time: 0.6402649879455566\n",
      "loss: 0.012915  [172800/204800] time: 0.64837646484375\n",
      "loss: 0.013187  [179200/204800] time: 0.4636702537536621\n",
      "loss: 0.013032  [185600/204800] time: 0.6041538715362549\n",
      "loss: 0.012908  [192000/204800] time: 0.7497565746307373\n",
      "loss: 0.035471  [198400/204800] time: 0.7118444442749023\n",
      "Valid | Error: \n",
      " Avg loss: 0.013380 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.013491  [    0/204800] time: 0.010739803314208984\n",
      "loss: 0.025627  [ 6400/204800] time: 0.7474758625030518\n",
      "loss: 0.013789  [12800/204800] time: 0.7683653831481934\n",
      "loss: 0.013172  [19200/204800] time: 0.7586896419525146\n",
      "loss: 0.013811  [25600/204800] time: 0.7632706165313721\n",
      "loss: 0.015258  [32000/204800] time: 0.7592039108276367\n",
      "loss: 0.029867  [38400/204800] time: 0.7607347965240479\n",
      "loss: 0.013271  [44800/204800] time: 0.7751092910766602\n",
      "loss: 0.020959  [51200/204800] time: 0.7631049156188965\n",
      "loss: 0.014356  [57600/204800] time: 0.7618553638458252\n",
      "loss: 0.026643  [64000/204800] time: 0.765373706817627\n",
      "loss: 0.013100  [70400/204800] time: 0.7672531604766846\n",
      "loss: 0.013224  [76800/204800] time: 0.766660213470459\n",
      "loss: 0.019307  [83200/204800] time: 0.765557050704956\n",
      "loss: 0.017613  [89600/204800] time: 0.765169620513916\n",
      "loss: 0.017126  [96000/204800] time: 0.7587864398956299\n",
      "loss: 0.014023  [102400/204800] time: 0.6849536895751953\n",
      "loss: 0.014401  [108800/204800] time: 0.6601085662841797\n",
      "loss: 0.012923  [115200/204800] time: 0.6721823215484619\n",
      "loss: 0.013008  [121600/204800] time: 0.787747859954834\n",
      "loss: 0.013284  [128000/204800] time: 0.7600288391113281\n",
      "loss: 0.013378  [134400/204800] time: 0.757770299911499\n",
      "loss: 0.013282  [140800/204800] time: 0.7638163566589355\n",
      "loss: 0.013416  [147200/204800] time: 0.7614948749542236\n",
      "loss: 0.021736  [153600/204800] time: 0.7585711479187012\n",
      "loss: 0.015860  [160000/204800] time: 0.7622244358062744\n",
      "loss: 0.014290  [166400/204800] time: 0.6952240467071533\n",
      "loss: 0.013796  [172800/204800] time: 0.4969501495361328\n",
      "loss: 0.013220  [179200/204800] time: 0.5234551429748535\n",
      "loss: 0.013662  [185600/204800] time: 0.5128002166748047\n",
      "loss: 0.034721  [192000/204800] time: 0.5109901428222656\n",
      "loss: 0.012891  [198400/204800] time: 0.5121262073516846\n",
      "Valid | Error: \n",
      " Avg loss: 0.012615 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.012683  [    0/204800] time: 0.011918783187866211\n",
      "loss: 0.012679  [ 6400/204800] time: 0.6417653560638428\n",
      "loss: 0.013019  [12800/204800] time: 0.6032578945159912\n",
      "loss: 0.015867  [19200/204800] time: 0.5039117336273193\n",
      "loss: 0.013510  [25600/204800] time: 0.4972348213195801\n",
      "loss: 0.060588  [32000/204800] time: 0.5057387351989746\n",
      "loss: 0.012776  [38400/204800] time: 0.5767810344696045\n",
      "loss: 0.012854  [44800/204800] time: 0.6300015449523926\n",
      "loss: 0.012880  [51200/204800] time: 0.7568211555480957\n",
      "loss: 0.012427  [57600/204800] time: 0.7555639743804932\n",
      "loss: 0.012704  [64000/204800] time: 0.5483944416046143\n",
      "loss: 0.012847  [70400/204800] time: 0.4562230110168457\n",
      "loss: 0.012672  [76800/204800] time: 0.457503080368042\n",
      "loss: 0.012856  [83200/204800] time: 0.4599435329437256\n",
      "loss: 0.013047  [89600/204800] time: 0.45719146728515625\n",
      "loss: 0.013241  [96000/204800] time: 0.45675086975097656\n",
      "loss: 0.013112  [102400/204800] time: 0.46153998374938965\n",
      "loss: 0.012931  [108800/204800] time: 0.4582662582397461\n",
      "loss: 0.012751  [115200/204800] time: 0.4570441246032715\n",
      "loss: 0.018303  [121600/204800] time: 0.4561910629272461\n",
      "loss: 0.045081  [128000/204800] time: 0.46216464042663574\n",
      "loss: 0.012837  [134400/204800] time: 0.45612382888793945\n",
      "loss: 0.014048  [140800/204800] time: 0.49906063079833984\n",
      "loss: 0.015036  [147200/204800] time: 0.4990379810333252\n",
      "loss: 0.012854  [153600/204800] time: 0.5041790008544922\n",
      "loss: 0.012800  [160000/204800] time: 0.508826732635498\n",
      "loss: 0.012859  [166400/204800] time: 0.5094814300537109\n",
      "loss: 0.012585  [172800/204800] time: 0.4821889400482178\n",
      "loss: 0.012181  [179200/204800] time: 0.4561471939086914\n",
      "loss: 0.012321  [185600/204800] time: 0.4599573612213135\n",
      "loss: 0.012223  [192000/204800] time: 0.4578549861907959\n",
      "loss: 0.012150  [198400/204800] time: 0.4584362506866455\n",
      "Valid | Error: \n",
      " Avg loss: 0.013278 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.013330  [    0/204800] time: 0.010996341705322266\n",
      "loss: 0.014452  [ 6400/204800] time: 0.5640223026275635\n",
      "loss: 0.017294  [12800/204800] time: 0.6390268802642822\n",
      "loss: 0.012422  [19200/204800] time: 0.7901740074157715\n",
      "loss: 0.012243  [25600/204800] time: 0.47931623458862305\n",
      "loss: 0.012359  [32000/204800] time: 0.47276902198791504\n",
      "loss: 0.012420  [38400/204800] time: 0.49732542037963867\n",
      "loss: 0.012356  [44800/204800] time: 0.492584228515625\n",
      "loss: 0.012387  [51200/204800] time: 0.4720470905303955\n",
      "loss: 0.018292  [57600/204800] time: 0.47045469284057617\n",
      "loss: 0.012557  [64000/204800] time: 0.5031321048736572\n",
      "loss: 0.012417  [70400/204800] time: 0.5210497379302979\n",
      "loss: 0.012574  [76800/204800] time: 0.7517094612121582\n",
      "loss: 0.613874  [83200/204800] time: 0.605736494064331\n",
      "loss: 0.012136  [89600/204800] time: 0.4967060089111328\n",
      "loss: 0.012338  [96000/204800] time: 0.5015678405761719\n",
      "loss: 0.012143  [102400/204800] time: 0.49886107444763184\n",
      "loss: 0.012199  [108800/204800] time: 0.5249001979827881\n",
      "loss: 0.012215  [115200/204800] time: 0.5088987350463867\n",
      "loss: 0.012197  [121600/204800] time: 0.49274492263793945\n",
      "loss: 0.012162  [128000/204800] time: 0.508284330368042\n",
      "loss: 0.012373  [134400/204800] time: 0.49956560134887695\n",
      "loss: 0.012309  [140800/204800] time: 0.5031418800354004\n",
      "loss: 0.012201  [147200/204800] time: 0.6874024868011475\n",
      "loss: 0.012256  [153600/204800] time: 0.590975284576416\n",
      "loss: 0.012312  [160000/204800] time: 0.497936487197876\n",
      "loss: 0.012166  [166400/204800] time: 0.5056517124176025\n",
      "loss: 0.012416  [172800/204800] time: 0.49480605125427246\n",
      "loss: 0.012453  [179200/204800] time: 0.5296621322631836\n",
      "loss: 0.012440  [185600/204800] time: 0.49805617332458496\n",
      "loss: 0.012528  [192000/204800] time: 0.4939408302307129\n",
      "loss: 0.012400  [198400/204800] time: 0.5142614841461182\n",
      "Valid | Error: \n",
      " Avg loss: 0.012449 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.012504  [    0/204800] time: 0.012186527252197266\n",
      "loss: 0.012518  [ 6400/204800] time: 0.48600125312805176\n",
      "loss: 0.012416  [12800/204800] time: 0.47968173027038574\n",
      "loss: 0.012467  [19200/204800] time: 0.4607083797454834\n",
      "loss: 0.014763  [25600/204800] time: 0.45841217041015625\n",
      "loss: 0.012653  [32000/204800] time: 0.45963501930236816\n",
      "loss: 0.012655  [38400/204800] time: 0.4587552547454834\n",
      "loss: 0.012545  [44800/204800] time: 0.456312894821167\n",
      "loss: 0.012586  [51200/204800] time: 0.46346616744995117\n",
      "loss: 0.012400  [57600/204800] time: 0.45534229278564453\n",
      "loss: 0.012462  [64000/204800] time: 0.4623556137084961\n",
      "loss: 0.012557  [70400/204800] time: 0.45560312271118164\n",
      "loss: 0.012531  [76800/204800] time: 0.4600541591644287\n",
      "loss: 0.012684  [83200/204800] time: 0.4559640884399414\n",
      "loss: 0.012706  [89600/204800] time: 0.4568164348602295\n",
      "loss: 0.012744  [96000/204800] time: 0.45641589164733887\n",
      "loss: 0.013219  [102400/204800] time: 0.4574878215789795\n",
      "loss: 0.012587  [108800/204800] time: 0.492584228515625\n",
      "loss: 0.012389  [115200/204800] time: 0.5155999660491943\n",
      "loss: 0.012507  [121600/204800] time: 0.5206048488616943\n",
      "loss: 0.012496  [128000/204800] time: 0.5323657989501953\n",
      "loss: 0.012443  [134400/204800] time: 0.5089538097381592\n",
      "loss: 0.012355  [140800/204800] time: 0.5045983791351318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.012504  [147200/204800] time: 0.5115950107574463\n",
      "loss: 0.012310  [153600/204800] time: 0.5037426948547363\n",
      "loss: 0.561690  [160000/204800] time: 0.49402332305908203\n",
      "loss: 0.012011  [166400/204800] time: 0.5612444877624512\n",
      "loss: 0.011973  [172800/204800] time: 0.5274324417114258\n",
      "loss: 0.011871  [179200/204800] time: 0.7867941856384277\n",
      "loss: 0.011993  [185600/204800] time: 0.7397429943084717\n",
      "loss: 0.011993  [192000/204800] time: 0.7725861072540283\n",
      "loss: 0.011985  [198400/204800] time: 0.6604053974151611\n",
      "Valid | Error: \n",
      " Avg loss: 0.011967 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.012048  [    0/204800] time: 0.012440919876098633\n",
      "loss: 0.013532  [ 6400/204800] time: 0.6598031520843506\n",
      "loss: 0.011941  [12800/204800] time: 0.6576707363128662\n",
      "loss: 6.735245  [19200/204800] time: 0.6412479877471924\n",
      "loss: 2.196992  [25600/204800] time: 0.6712527275085449\n",
      "loss: 0.692562  [32000/204800] time: 0.6806306838989258\n",
      "loss: 0.213095  [38400/204800] time: 0.6922240257263184\n",
      "loss: 0.068066  [44800/204800] time: 0.7898032665252686\n",
      "loss: 0.026594  [51200/204800] time: 0.6471004486083984\n",
      "loss: 0.015688  [57600/204800] time: 0.7564549446105957\n",
      "loss: 0.012727  [64000/204800] time: 0.7878384590148926\n",
      "loss: 0.012168  [70400/204800] time: 0.6048853397369385\n",
      "loss: 0.011992  [76800/204800] time: 0.5152802467346191\n",
      "loss: 0.011996  [83200/204800] time: 0.5514106750488281\n",
      "loss: 0.011960  [89600/204800] time: 0.45569729804992676\n",
      "loss: 0.011934  [96000/204800] time: 0.46474456787109375\n",
      "loss: 0.011903  [102400/204800] time: 0.45725131034851074\n",
      "loss: 0.011878  [108800/204800] time: 0.4605832099914551\n",
      "loss: 0.011913  [115200/204800] time: 0.45846986770629883\n",
      "loss: 0.012045  [121600/204800] time: 0.4586751461029053\n",
      "loss: 0.012047  [128000/204800] time: 0.4588165283203125\n",
      "loss: 0.011916  [134400/204800] time: 0.45736145973205566\n",
      "loss: 0.012110  [140800/204800] time: 0.5895054340362549\n",
      "loss: 0.011876  [147200/204800] time: 0.455486536026001\n",
      "loss: 0.012036  [153600/204800] time: 0.4637911319732666\n",
      "loss: 0.011923  [160000/204800] time: 0.4672849178314209\n",
      "loss: 0.011851  [166400/204800] time: 0.4654233455657959\n",
      "loss: 0.012039  [172800/204800] time: 0.46586012840270996\n",
      "loss: 0.011881  [179200/204800] time: 0.46498918533325195\n",
      "loss: 0.011936  [185600/204800] time: 0.46103930473327637\n",
      "loss: 0.011798  [192000/204800] time: 0.4611330032348633\n",
      "loss: 0.012208  [198400/204800] time: 0.45694661140441895\n",
      "Valid | Error: \n",
      " Avg loss: 0.011954 \n",
      "\n",
      "Done!\n",
      "Test | Error: \n",
      " Avg loss: 0.011957 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.run_autoencoder_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24814ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3614, 0.4291, 0.4119, 0.5980, 0.5786, 0.5185, 0.4565, 0.4176, 0.4460,\n",
      "        0.6861, 0.5941, 0.4609, 0.4273, 0.5244, 0.5315, 0.6967, 0.5909, 0.5472,\n",
      "        0.5972, 0.4758, 0.5279, 0.6598, 0.7253, 0.5215, 0.5445, 0.6050, 0.5264,\n",
      "        0.4906, 0.5735, 0.4125, 0.6397, 0.4163, 0.2822, 0.3683, 0.2601, 0.5341,\n",
      "        0.4667, 0.5159, 0.5163, 0.3000, 0.4588, 0.4980, 0.4076, 0.2685, 0.5867,\n",
      "        0.5801, 0.6160, 0.7872, 0.4398, 0.5212, 0.6395, 0.5187, 0.5020, 0.4134,\n",
      "        0.5451, 0.4044, 0.6065, 0.3978, 0.7233, 0.4681, 0.7729, 0.1948, 0.4291,\n",
      "        0.6155, 0.4201, 0.6287, 0.5473, 0.4003, 0.6670, 0.7474, 0.5876, 0.5277,\n",
      "        0.4395, 0.6778, 0.4443, 0.4961, 0.5346, 0.3843, 0.7089, 0.3440, 0.3573,\n",
      "        0.4817, 0.5170, 0.5062, 0.4475, 0.5910, 0.4525, 0.5091, 0.4880, 0.4911,\n",
      "        0.5237, 0.4277, 0.7582, 0.3470, 0.4931, 0.3763, 0.6141, 0.4815, 0.4021,\n",
      "        0.5947, 0.2465, 0.3480, 0.5340, 0.6162, 0.6578, 0.4116, 0.5074, 0.5673,\n",
      "        0.3086, 0.4053, 0.5559, 0.3769, 0.4246, 0.4384, 0.5225, 0.4546, 0.3527,\n",
      "        0.5860, 0.5391, 0.5726, 0.5707, 0.5103, 0.6115, 0.4220, 0.3933, 0.6022,\n",
      "        0.5020, 0.4886, 0.8064, 0.3276, 0.6230, 0.4425, 0.5722, 0.4836, 0.6604,\n",
      "        0.5354, 0.5586, 0.3513, 0.5419, 0.4848, 0.4434, 0.4364, 0.3075, 0.3221,\n",
      "        0.6372, 0.6194, 0.6719, 0.5926, 0.4027, 0.5228, 0.5582, 0.5577, 0.3616,\n",
      "        0.3378, 0.4605, 0.4071, 0.6032, 0.5771, 0.5781, 0.4887, 0.5015, 0.6475,\n",
      "        0.4872, 0.5072, 0.5118, 0.6787, 0.4194, 0.6814, 0.6267, 0.4381, 0.6176,\n",
      "        0.5880, 0.3501, 0.3692, 0.5051, 0.3818, 0.5061, 0.4811, 0.4536, 0.4018,\n",
      "        0.5856, 0.5749, 0.5292, 0.3376, 0.4602, 0.4664, 0.5215, 0.6032, 0.5778,\n",
      "        0.5415, 0.5871, 0.3338, 0.6233, 0.3964, 0.2700, 0.4001, 0.4548, 0.4014,\n",
      "        0.4943, 0.3708, 0.6829, 0.6399, 0.2451, 0.4707, 0.6387, 0.5248, 0.5120,\n",
      "        0.4065, 0.4894, 0.4332, 0.4856, 0.6180, 0.7265, 0.5635, 0.5577, 0.4402,\n",
      "        0.6407, 0.5443, 0.5136, 0.5745, 0.5708, 0.6043, 0.4821, 0.5696, 0.3525,\n",
      "        0.4090, 0.6197, 0.5514, 0.5521, 0.6056, 0.3955, 0.5485, 0.4090, 0.6775,\n",
      "        0.5147, 0.4618, 0.5675, 0.5771, 0.5146, 0.3944, 0.5515, 0.5000, 0.4634,\n",
      "        0.3125, 0.4427, 0.5041, 0.5737, 0.3904, 0.3687, 0.5026, 0.5105, 0.6005,\n",
      "        0.5264, 0.4071, 0.6205, 0.4100, 0.5279, 0.4135, 0.3754, 0.3513, 0.4369,\n",
      "        0.5354, 0.3316, 0.4585, 0.4553, 0.3328, 0.3467, 0.4409, 0.5522, 0.5089,\n",
      "        0.4449, 0.6821, 0.5401, 0.3729, 0.5182, 0.5491, 0.3123, 0.5885, 0.4781,\n",
      "        0.4455, 0.5161, 0.4603, 0.7276, 0.2788, 0.5384, 0.4316, 0.2736, 0.5464,\n",
      "        0.3863, 0.4352, 0.2987, 0.4732, 0.5913, 0.4641, 0.2405, 0.6628, 0.4207,\n",
      "        0.5041, 0.4996, 0.5542, 0.5730, 0.6943, 0.5812, 0.4623, 0.5700, 0.3342,\n",
      "        0.3092, 0.4047, 0.5353, 0.7123, 0.4778, 0.4035, 0.6066, 0.5872, 0.4517,\n",
      "        0.6438, 0.5943, 0.4598, 0.6036, 0.6908, 0.5820, 0.3558, 0.3537, 0.6081,\n",
      "        0.4782, 0.5779, 0.3461, 0.4866, 0.4544, 0.4809, 0.5009, 0.4537, 0.4189,\n",
      "        0.5397, 0.6659, 0.5853, 0.6344, 0.5049, 0.6235, 0.5165, 0.4331, 0.6106,\n",
      "        0.5160, 0.3438, 0.5112, 0.5875, 0.7449, 0.3785, 0.4061, 0.4594, 0.5754,\n",
      "        0.5508, 0.3706, 0.7312, 0.4423, 0.6116, 0.5992, 0.4752, 0.6802, 0.5400,\n",
      "        0.3927, 0.6908, 0.4745, 0.3995, 0.5976, 0.3325, 0.5266, 0.3143, 0.4807,\n",
      "        0.6672, 0.3764, 0.6392, 0.3664, 0.5719, 0.6284, 0.6061, 0.4432, 0.6726,\n",
      "        0.5920, 0.5278, 0.2127, 0.5604, 0.5261, 0.3447, 0.4877, 0.4318, 0.4981,\n",
      "        0.6091, 0.3694, 0.6367, 0.4348, 0.3984, 0.4801, 0.5927, 0.5418, 0.4209,\n",
      "        0.5971, 0.5173, 0.3633, 0.5098, 0.7340, 0.5040, 0.4650, 0.3077, 0.6063,\n",
      "        0.4978, 0.3584, 0.3130, 0.5474, 0.5226, 0.6999, 0.5157, 0.5412, 0.4460,\n",
      "        0.2159, 0.4398, 0.6054, 0.3908, 0.6045, 0.3230, 0.5238, 0.6085, 0.5135,\n",
      "        0.6548, 0.4417, 0.4167, 0.4094, 0.4022, 0.3463, 0.3092, 0.4263, 0.4874,\n",
      "        0.5890, 0.5288, 0.4726, 0.5173, 0.4915, 0.5057, 0.5755, 0.4391, 0.5548,\n",
      "        0.4903, 0.4718, 0.5925, 0.4853, 0.4837, 0.4216, 0.5912, 0.5352, 0.4823,\n",
      "        0.2534, 0.3361, 0.5045, 0.3234, 0.5948, 0.3766, 0.2586, 0.5770, 0.4946,\n",
      "        0.2147, 0.5154, 0.2329, 0.3632, 0.4491, 0.5151, 0.4612, 0.5805, 0.6243,\n",
      "        0.5263, 0.3010, 0.7327, 0.5248, 0.6375, 0.7692, 0.5743, 0.2933, 0.5324,\n",
      "        0.5494, 0.5222, 0.2969, 0.6311, 0.5194, 0.4799, 0.4672, 0.5186, 0.3741,\n",
      "        0.4801, 0.4553, 0.3089, 0.4344, 0.5638, 0.4556, 0.7233, 0.6320, 0.3303,\n",
      "        0.4551, 0.5438, 0.2718, 0.5354, 0.6041, 0.5015, 0.6258, 0.3698, 0.6047,\n",
      "        0.5438, 0.3832, 0.4164, 0.6338, 0.4203, 0.4428, 0.5545, 0.5601])\n",
      "tensor([0.5087, 0.4945, 0.4829, 0.4820, 0.4999, 0.4893, 0.5211, 0.4904, 0.4915,\n",
      "        0.4948, 0.4859, 0.5076, 0.5039, 0.4987, 0.5091, 0.4950, 0.4966, 0.5274,\n",
      "        0.5190, 0.5057, 0.4640, 0.4826, 0.4622, 0.5158, 0.5013, 0.4966, 0.5032,\n",
      "        0.5115, 0.4858, 0.5080, 0.5497, 0.5042, 0.4935, 0.4637, 0.5260, 0.5114,\n",
      "        0.5148, 0.5091, 0.5116, 0.4755, 0.4675, 0.5082, 0.5337, 0.4886, 0.4720,\n",
      "        0.5359, 0.5082, 0.5189, 0.5011, 0.4864, 0.5020, 0.4818, 0.4702, 0.5182,\n",
      "        0.4731, 0.5216, 0.5236, 0.5053, 0.5074, 0.5188, 0.5368, 0.4919, 0.5026,\n",
      "        0.4972, 0.5370, 0.4942, 0.5106, 0.4675, 0.5287, 0.4970, 0.4725, 0.5221,\n",
      "        0.5165, 0.5517, 0.4994, 0.5162, 0.4893, 0.4877, 0.4907, 0.4687, 0.5029,\n",
      "        0.5039, 0.4826, 0.4919, 0.4741, 0.4825, 0.4926, 0.4942, 0.4560, 0.5021,\n",
      "        0.5116, 0.4851, 0.5261, 0.4575, 0.4733, 0.5462, 0.5146, 0.5300, 0.4773,\n",
      "        0.4931, 0.4757, 0.5087, 0.5304, 0.4936, 0.5242, 0.5167, 0.4900, 0.5054,\n",
      "        0.4752, 0.5082, 0.4753, 0.5137, 0.4876, 0.4697, 0.5003, 0.4705, 0.4698,\n",
      "        0.5031, 0.5123, 0.5338, 0.5063, 0.4828, 0.4846, 0.4851, 0.5185, 0.4699,\n",
      "        0.4824, 0.5122, 0.4825, 0.5283, 0.4870, 0.5075, 0.4805, 0.4677, 0.5073,\n",
      "        0.5061, 0.5083, 0.5126, 0.4858, 0.5243, 0.5046, 0.4806, 0.5116, 0.5070,\n",
      "        0.5516, 0.4851, 0.5160, 0.4926, 0.5252, 0.5559, 0.5080, 0.5117, 0.4814,\n",
      "        0.5335, 0.5025, 0.4816, 0.5133, 0.5194, 0.4760, 0.4727, 0.5010, 0.4875,\n",
      "        0.4671, 0.4754, 0.4699, 0.4916, 0.5093, 0.5093, 0.5040, 0.5044, 0.5424,\n",
      "        0.4977, 0.4888, 0.5029, 0.5214, 0.5003, 0.5054, 0.4927, 0.4725, 0.4554,\n",
      "        0.5265, 0.5290, 0.5132, 0.4995, 0.5302, 0.5033, 0.4947, 0.4748, 0.5018,\n",
      "        0.4912, 0.5091, 0.4619, 0.5080, 0.4627, 0.5157, 0.5077, 0.4850, 0.4813,\n",
      "        0.5025, 0.5197, 0.5419, 0.5043, 0.5131, 0.4992, 0.5129, 0.4858, 0.4877,\n",
      "        0.5013, 0.4892, 0.5256, 0.4761, 0.5080, 0.5518, 0.4865, 0.5174, 0.5175,\n",
      "        0.5106, 0.4880, 0.4422, 0.4727, 0.5010, 0.4959, 0.4806, 0.5114, 0.5046,\n",
      "        0.5136, 0.4761, 0.5001, 0.4799, 0.4961, 0.4836, 0.4862, 0.4998, 0.5244,\n",
      "        0.4910, 0.4497, 0.5338, 0.5552, 0.5216, 0.4738, 0.4922, 0.5295, 0.4737,\n",
      "        0.4784, 0.4954, 0.5008, 0.4946, 0.5021, 0.4857, 0.5052, 0.4835, 0.5028,\n",
      "        0.5304, 0.4937, 0.5228, 0.5007, 0.5307, 0.5341, 0.5249, 0.4788, 0.4963,\n",
      "        0.5212, 0.4940, 0.4737, 0.4635, 0.4983, 0.4938, 0.4903, 0.5416, 0.5033,\n",
      "        0.4979, 0.5128, 0.4965, 0.5024, 0.5281, 0.5407, 0.5012, 0.5258, 0.5249,\n",
      "        0.4869, 0.5088, 0.4895, 0.4742, 0.4901, 0.4697, 0.5054, 0.4965, 0.5235,\n",
      "        0.5192, 0.5163, 0.4694, 0.4805, 0.4934, 0.4519, 0.4963, 0.5148, 0.5254,\n",
      "        0.4989, 0.4999, 0.5050, 0.5347, 0.5016, 0.5001, 0.5006, 0.4847, 0.5168,\n",
      "        0.4844, 0.4873, 0.5011, 0.5181, 0.4977, 0.4836, 0.5010, 0.4659, 0.4965,\n",
      "        0.4875, 0.4888, 0.5215, 0.4562, 0.5198, 0.5026, 0.4938, 0.5159, 0.5322,\n",
      "        0.5045, 0.5258, 0.4997, 0.4996, 0.4951, 0.5113, 0.5181, 0.4843, 0.4733,\n",
      "        0.4726, 0.5086, 0.5074, 0.5050, 0.4882, 0.5437, 0.5061, 0.4761, 0.5364,\n",
      "        0.4904, 0.4907, 0.5136, 0.4636, 0.5048, 0.5485, 0.5067, 0.5116, 0.4925,\n",
      "        0.5131, 0.5134, 0.4939, 0.5142, 0.5025, 0.5300, 0.5155, 0.5104, 0.5057,\n",
      "        0.4991, 0.5095, 0.4901, 0.4924, 0.4840, 0.5142, 0.4933, 0.4887, 0.4607,\n",
      "        0.5296, 0.4915, 0.5477, 0.5024, 0.4985, 0.5035, 0.4867, 0.5015, 0.5162,\n",
      "        0.4966, 0.5102, 0.4969, 0.4688, 0.5085, 0.5309, 0.5097, 0.5004, 0.4953,\n",
      "        0.4893, 0.4939, 0.4986, 0.5385, 0.4858, 0.4925, 0.5316, 0.5107, 0.4685,\n",
      "        0.5478, 0.4782, 0.4799, 0.4963, 0.5078, 0.5105, 0.4962, 0.4827, 0.4697,\n",
      "        0.5020, 0.4892, 0.5056, 0.5075, 0.5047, 0.4871, 0.4944, 0.5010, 0.4851,\n",
      "        0.4771, 0.5197, 0.5119, 0.5088, 0.4800, 0.4972, 0.4810, 0.4829, 0.5103,\n",
      "        0.4959, 0.5035, 0.5001, 0.5144, 0.4968, 0.4847, 0.4533, 0.5040, 0.4810,\n",
      "        0.5202, 0.5194, 0.5083, 0.4784, 0.4806, 0.5009, 0.5524, 0.4801, 0.5167,\n",
      "        0.4858, 0.5117, 0.4757, 0.4529, 0.4570, 0.5044, 0.5115, 0.5360, 0.5288,\n",
      "        0.5031, 0.5207, 0.5216, 0.4980, 0.5383, 0.4823, 0.4942, 0.4928, 0.5316,\n",
      "        0.4688, 0.5031, 0.5147, 0.5100, 0.4762, 0.5155, 0.5026, 0.4994, 0.5001,\n",
      "        0.5097, 0.5126, 0.5198, 0.4906, 0.4745, 0.4931, 0.5369, 0.5073, 0.4887,\n",
      "        0.4885, 0.4633, 0.4948, 0.5026, 0.5295, 0.4665, 0.5049, 0.5117, 0.4844,\n",
      "        0.4686, 0.5132, 0.4849, 0.5014, 0.4947, 0.5100, 0.5426, 0.5113, 0.5086,\n",
      "        0.4691, 0.4931, 0.5081, 0.4963, 0.5175, 0.5100, 0.5004, 0.5010, 0.4681,\n",
      "        0.4934, 0.5049, 0.4868, 0.4818, 0.5351, 0.4837, 0.5038, 0.5060],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4684/2774105925.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pred = model(torch.tensor(test).to('cuda'))\n"
     ]
    }
   ],
   "source": [
    "test = test_data[1][0]\n",
    "print(test)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(torch.tensor(test).to('cuda'))\n",
    "    \n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c79eb358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0022)\n"
     ]
    }
   ],
   "source": [
    "print((test - pred.cpu()).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50ab09cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stylegan_generator import StyleGANGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "782d2908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9363b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
